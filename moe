# MoE(Mixture of Experts)
- 인공지능에서 여러 개의 전문가 모델(Expert)과 게이팅 네트워크(Gating Network)를 결합해, 입력마다 최적으로 전문가를 선택하고 부분적으로만 연산을 수행하는 구조

## 기본 구조 및 동작
1. 전문가(Expert) 네트워크: 각 하위 모델은 특정 유형의 입력이나 데이터 패턴에 특화된 신경망. 일반적으로 MLP(다층 퍼셉트론) 또는 Transformer 레이어로 구현
2. 게이팅 네트워크(Gating Network): 입력을 받아 그에 적합한 전문가(들)를 선택 -> Softmax 또는 Top-K 방식 등으로 여러 전문가에 점수를 부여하고 일부만 활성화
3. 출력 집계: 선택된 전문가들의 출력을 가중합해서 최종적으로 예측

## 특징
1. 많은 파라미터를 사용하더라도 연산량이 비교적 크지 않음(데이터나 입력의 특성에 맞춰 전문가를 동적으로 할당 -> 필요한 전문가만으로 계산)
2. 게이팅과 전문가 분산, 부하 불균형 등 관리 난이도가 있음
3. 요청에 따른 결과가 어떤 전문가가 선택되었는지 설명하기 어려움

## 참조
1. Mixtral 모델 : https://huggingface.co/mistralai/Mixtral-8x7B-v0.1
2. Switch Transformer : https://arxiv.org/abs/2101.03961
